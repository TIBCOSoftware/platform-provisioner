#
# Copyright Â© 2024. Cloud Software Group, Inc.
# This file is subject to the license terms contained
# in the license file that is distributed with this file.
#

# Recipe for installing TIBCO Platform EKS
apiVersion: v1
kind: helm-install
meta:
  globalEnvVariable:
    # piepline env
    REPLACE_RECIPE: true
    PIPELINE_LOG_DEBUG: false
    PIPELINE_CHECK_DOCKER_STATUS: false
    # github
    GITHUB_TOKEN: ${GUI_GITHUB_TOKEN}
    TP_CHART_REPO: https://tibcosoftware.github.io/tp-helm-charts
    # cluster
    TP_CLUSTER_NAME: ${GUI_TP_CLUSTER_NAME}
    TP_CLUSTER_VERSION: ${GUI_TP_CLUSTER_VERSION:-1.29}
    TP_CLUSTER_REGION: ${AWS_REGION:-us-west-2}
    # domain
    TP_DOMAIN: ${GUI_TP_DOMAIN} # the star domain for the main ingress <cp/dp-env>.${SANDBOX}.dataplanes.pro
    TP_CLUSTER_VPC_CIDR: "10.180.0.0/16"
    # ingress
    TP_MAIN_INGRESS_CONTROLLER: alb # traefik or traefik, alb for AWS ingress controller
    TP_INGRESS_CLASS: ${GUI_TP_INGRESS_CLASS:-"nginx"}
    TP_INGRESS_RELEASE_NAME: dp-config-aws-ingress
    TP_ES_RELEASE_NAME: dp-config-es
    # network policy
    TP_CLUSTER_ENABLE_NETWORK_POLICY: ${GUI_TP_CLUSTER_ENABLE_NETWORK_POLICY:-true}
    # storage
    TP_EBS_ENABLED: true
    TP_EFS_ENABLED: true
    TP_STORAGE_CLASS: ebs-gp3
    TP_STORAGE_CLASS_EFS: efs-sc
    TP_INSTALL_RESOURCE_FOLDER: "/workspace/resources"
    TP_INSTALL_EFS_VALUES_FILE: "${TP_INSTALL_RESOURCE_FOLDER}/efs_values.yaml"
    # flow control
    TP_INSTALL_K8S: true # change to true to install k8s
    TP_INSTALL_EFS: true # change to true to install efs
    TP_INSTALL_STORAGE: true # change to false to skip storage installation
    TP_SCRIPT_BRANCH: main
    TP_SCRIPT_NAME_SH_EKS: create-eks.sh # the script that DP will run
    TP_SCRIPT_NAME_SH_EFS: create-efs.sh # the script that DP will run
    TP_INSTALL_POSTGRES: ${GUI_TP_INSTALL_POSTGRES:-true}
    TP_ENABLE_O11Y_DAEMONSET: ${GUI_TP_ENABLE_O11Y_DAEMONSET:-true}
    # Do not change, variables to configure nginx/kong related resources based on ingressclass passed
    TP_ENABLE_NGINX: $([[ "$TP_INGRESS_CLASS" == "nginx" ]] && echo "true" || echo "false")
    TP_ENABLE_KONG: $([[ "$TP_INGRESS_CLASS" == "kong" ]] && echo "true" || echo "false")
    TP_HTTP_INGRESS_BACKEND_SERVICE: ${TP_INGRESS_RELEASE_NAME}-$( ( [[  "$TP_INGRESS_CLASS" == "nginx" ]] && echo "ingress-nginx-controller" ) || ( [[ "$TP_INGRESS_CLASS" == "kong" ]] && echo "kong-proxy" ) )
    TP_INSTALL_O11Y: ${GUI_TP_INSTALL_O11Y:-false}
  tools:
    yq: "4.40"
    helm: "3.13"
    kubectl: "1.28"
preTasks:
- condition: ${TP_INSTALL_K8S}
  repo:
    git:
      github:
        repo: github.com/TIBCOSoftware/platform-provisioner
        path: docs/recipes/k8s/cloud/scripts/eks
        branch: ${TP_SCRIPT_BRANCH}
  script:
    ignoreErrors: false
    fileName: ${TP_SCRIPT_NAME_SH_EKS}
- condition: ${TP_INSTALL_EFS}
  clusters:
    - name: ${TP_CLUSTER_NAME}
  repo:
    git:
      github:
        repo: github.com/TIBCOSoftware/platform-provisioner
        path: docs/recipes/k8s/cloud/scripts/eks
        branch: ${TP_SCRIPT_BRANCH}
  script:
    ignoreErrors: false
    fileName: ${TP_SCRIPT_NAME_SH_EFS}
helmCharts:
- name: cert-manager
  version: v1.13.2
  repo:
    helm:
      url: https://charts.jetstack.io
  values:
    keepPrevious: true
    content: |
      installCRDs: true
      serviceAccount:
        create: false
        name: cert-manager
  cluster:
    names:
      - ${TP_CLUSTER_NAME}
  releaseName: cert-manager
  namespace: cert-manager
  flags:
    wait: true
    timeout: 1h
    createNamespace: true
- name: external-dns
  version: 1.13.1
  repo:
    helm:
      url: https://kubernetes-sigs.github.io/external-dns/
  values:
    keepPrevious: true
    content: |
      serviceAccount:
        create: false
        name: external-dns 
      extraArgs:
        # add filter to only sync only public Ingresses with this annotation
        - "--annotation-filter=kubernetes.io/ingress.class=${TP_MAIN_INGRESS_CONTROLLER}"
  cluster:
    names:
      - ${TP_CLUSTER_NAME}
  releaseName: external-dns
  namespace: external-dns-system
  flags:
    wait: true
    timeout: 1h
    createNamespace: true
- name: aws-load-balancer-controller
  version: 1.6.2
  repo:
    helm:
      url: https://aws.github.io/eks-charts
  values:
    keepPrevious: true
    content: |
      clusterName: ${TP_CLUSTER_NAME}
      serviceAccount:
        create: false
        name: aws-load-balancer-controller
  cluster:
    names:
      - ${TP_CLUSTER_NAME}
  releaseName: aws-load-balancer-controller
  namespace: kube-system
  flags:
    wait: true
    timeout: 1h
    createNamespace: true
- name: metrics-server
  version: "3.11.0"
  repo:
    helm:
      url: https://kubernetes-sigs.github.io/metrics-server/
  values:
    keepPrevious: true
    content: |
      clusterName: ${TP_CLUSTER_NAME}
      serviceAccount:
        create: true
        name: metrics-server
  cluster:
    names:
      - ${TP_CLUSTER_NAME}
  releaseName: metrics-server
  namespace: kube-system
  flags:
    wait: true
    timeout: 1h
    createNamespace: true
- name: dp-config-aws
  version: "1.0.25"
  repo:
    helm:
      url: ${TP_CHART_REPO}
  values:
    keepPrevious: false
    content: |
      dns:
        domain: "${TP_DOMAIN}"
      httpIngress:
        enabled: true
        name: ${TP_INGRESS_CLASS}
        backend:
          serviceName: ${TP_HTTP_INGRESS_BACKEND_SERVICE}
        annotations:
          alb.ingress.kubernetes.io/group.name: "${TP_DOMAIN}"
          external-dns.alpha.kubernetes.io/hostname: "*.${TP_DOMAIN}"
          # this will be used for external-dns annotation filter
          kubernetes.io/ingress.class: alb
      ingress-nginx:
        enabled: ${TP_ENABLE_NGINX}
        controller:
          config:
            use-forwarded-headers: 'true'          # PDP-945
      kong:
        enabled: ${TP_ENABLE_KONG}
  cluster:
    names:
      - ${TP_CLUSTER_NAME}
  releaseName: ${TP_INGRESS_RELEASE_NAME}
  namespace: ingress-system
  flags:
    wait: true
    timeout: 1h
    createNamespace: true
- name: dp-config-aws
  version: "1.0.25"
  condition: ${TP_INSTALL_STORAGE}
  repo:
    helm:
      url: ${TP_CHART_REPO}
  values:
    keepPrevious: false
    content: |
      httpIngress:
        enabled: false
      ingress-nginx:
        enabled: false
      service:
        enabled: false
      storageClass:
        ebs:
          enabled: ${TP_EBS_ENABLED}
        efs:
          enabled: ${TP_EFS_ENABLED}
  cluster:
    names:
      - ${TP_CLUSTER_NAME}
  releaseName: dp-config-aws-storage
  namespace: storage-system
  flags:
    wait: true
    timeout: 1h
    createNamespace: true
    extra: "--values ${TP_INSTALL_EFS_VALUES_FILE}"
- name: postgresql
  version: 11.9.13 # 14.3.3 use postgresql 16.2.0, 11.9.13 use postgresql 14.5.0 PCP-4922
  namespace: tibco-ext
  releaseName: postgresql
  condition: ${TP_INSTALL_POSTGRES}
  repo:
    helm:
      url: https://charts.bitnami.com/bitnami
  cluster:
    names:
      - ${TP_CLUSTER_NAME}
  values:
    keepPrevious: true
    content: |
      auth:
        postgresPassword: postgres
        username: postgres
        password: postgres
        database: "postgres"
      persistence:
        storageClass: ${TP_STORAGE_CLASS}
  flags:
    createNamespace: true
    timeout: 1h
- name: eck-operator
  version: 2.10.0
  condition: ${TP_INSTALL_O11Y}
  repo:
    helm:
      url: https://helm.elastic.co
  cluster:
    names:
      - ${TP_CLUSTER_NAME}
  releaseName: eck-operator
  namespace: elastic-system
  flags:
    wait: true
    timeout: 1h
    createNamespace: true
- name: dp-config-es
  version: "1.0.17"
  condition: ${TP_INSTALL_O11Y}
  repo:
    helm:
      url: ${TP_CHART_REPO}
  values:
    keepPrevious: true
    content: |
      domain: ${TP_DOMAIN}
      es:
        version: "8.11.1"
        ingress:
          ingressClassName: ${TP_INGRESS_CLASS}
          service: ${TP_ES_RELEASE_NAME}-es-http
        storage:
          name: ${TP_STORAGE_CLASS}
      kibana:
        version: "8.11.1"
        ingress:
          ingressClassName: ${TP_INGRESS_CLASS}
          service: ${TP_ES_RELEASE_NAME}-kb-http
      apm:
        enabled: true
        version: "8.11.1"
        ingress:
          ingressClassName: ${TP_INGRESS_CLASS}
          service: ${TP_ES_RELEASE_NAME}-apm-http
  cluster:
    names:
      - ${TP_CLUSTER_NAME}
  releaseName: ${TP_ES_RELEASE_NAME}
  namespace: elastic-system
  flags:
    wait: true
    timeout: 1h
    createNamespace: true
- name: kube-prometheus-stack
  version: "54.2.1"
  condition: ${TP_INSTALL_O11Y}
  repo:
    helm:
      url: https://prometheus-community.github.io/helm-charts
  values:
    keepPrevious: true
    content: |
      grafana:
        plugins:
          - grafana-piechart-panel
        ingress:
          enabled: true
          ingressClassName: ${TP_INGRESS_CLASS}
          hosts:
          - grafana.${TP_DOMAIN}
      prometheus:
        prometheusSpec:
          enableRemoteWriteReceiver: true
          remoteWriteDashboards: true
          additionalScrapeConfigs:
          - job_name: otel-collector
            kubernetes_sd_configs:
            - role: pod
            relabel_configs:
            - action: keep
              regex: "true"
              source_labels:
              - __meta_kubernetes_pod_label_prometheus_io_scrape
            - action: keep
              regex: "infra"
              source_labels:
              - __meta_kubernetes_pod_label_platform_tibco_com_workload_type
            - action: keepequal
              source_labels: [__meta_kubernetes_pod_container_port_number]
              target_label: __meta_kubernetes_pod_label_prometheus_io_port
            - action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              source_labels:
              - __address__
              - __meta_kubernetes_pod_label_prometheus_io_port
              target_label: __address__
            - source_labels: [__meta_kubernetes_pod_label_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
              replacement: /$1
        ingress:
          enabled: true
          ingressClassName: ${TP_INGRESS_CLASS}
          hosts:
          - prometheus-internal.${TP_DOMAIN}
  cluster:
    names:
      - ${TP_CLUSTER_NAME}
  releaseName: kube-prometheus-stack
  namespace: prometheus-system
  flags:
    wait: true
    timeout: 1h
    createNamespace: true
- name: opentelemetry-collector
  version: "0.74.1"
  condition: ${TP_ENABLE_O11Y_DAEMONSET}
  repo:
    helm:
      url: https://open-telemetry.github.io/opentelemetry-helm-charts
  values:
    keepPrevious: true
    content: |
      mode: "daemonset"
      fullnameOverride: otel-kubelet-stats
      podLabels:
        platform.tibco.com/workload-type: "infra"
        networking.platform.tibco.com/kubernetes-api: enable
        egress.networking.platform.tibco.com/internet-all: enable
        prometheus.io/scrape: "true"
        prometheus.io/path: "metrics"
        prometheus.io/port: "4319"
      autoscaling:
        enabled: false
        minReplicas: 1
        maxReplicas: 10
        behavior:
          scaleUp:
            stabilizationWindowSeconds: 15
          scaleDown:
            stabilizationWindowSeconds: 15
        targetCPUUtilizationPercentage: 80
        targetMemoryUtilizationPercentage: 80
      serviceAccount:
        create: true
      clusterRole:
        create: true
        rules:
        - apiGroups: [""]
          resources: ["pods", "namespaces"]
          verbs: ["get", "watch", "list"]
        - apiGroups: [""]
          resources: ["nodes/stats", "nodes/proxy"]
          verbs: ["get"]
      extraEnvs:
        - name: KUBE_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
      ports:
        metrics:
          enabled: true
          containerPort: 8888
          servicePort: 8888
          hostPort: 8888
          protocol: TCP
        prometheus:
          enabled: true
          containerPort: 4319
          servicePort: 4319
          hostPort: 4319
          protocol: TCP
      config:
        receivers:
          kubeletstats:
            collection_interval: 20s
            auth_type: "serviceAccount"
            endpoint: "https://${env:KUBE_NODE_NAME}:10250"
            insecure_skip_verify: true
            metric_groups:
              - pod
              - container
            extra_metadata_labels:
              - container.id
            metrics:
              k8s.container.memory_limit_utilization:
                enabled: true
              k8s.container.cpu_limit_utilization:
                enabled: true
              k8s.pod.cpu_limit_utilization:
                enabled: true
              k8s.pod.memory_limit_utilization:
                enabled: true
              k8s.pod.filesystem.available:
                enabled: false
              k8s.pod.filesystem.capacity:
                enabled: false
              k8s.pod.filesystem.usage:
                enabled: false
              k8s.pod.memory.major_page_faults:
                enabled: false
              k8s.pod.memory.page_faults:
                enabled: false
              k8s.pod.memory.rss:
                enabled: false
              k8s.pod.memory.working_set:
                enabled: false
        processors:
          memory_limiter:
            check_interval: 5s
            limit_percentage: 80
            spike_limit_percentage: 25
          batch: {}
          k8sattributes/kubeletstats:
            auth_type: "serviceAccount"
            passthrough: false
            extract:
              metadata:
                - k8s.pod.name
                - k8s.pod.uid
                - k8s.namespace.name
                - k8s.pod.start_time
              annotations:
                - tag_name: connectors
                  key: platform.tibco.com/connectors
                  from: pod
              labels:
                - tag_name: app_id
                  key: platform.tibco.com/app-id
                  from: pod
                - tag_name: app_type
                  key: platform.tibco.com/app-type
                  from: pod
                - tag_name: dataplane_id
                  key: platform.tibco.com/dataplane-id
                  from: pod
                - tag_name: workload_type
                  key: platform.tibco.com/workload-type
                  from: pod
                - tag_name: app_name
                  key: platform.tibco.com/app-name
                  from: pod
                - tag_name: app_version
                  key: platform.tibco.com/app-version
                  from: pod
                - tag_name: app_tags
                  key: platform.tibco.com/tags
                  from: pod
                - tag_name: capability_instance_id
                  key: platform.tibco.com/capability-instance-id
                  from: pod
                - tag_name: tib-msg-stsrole
                  key: tib-msg-stsrole
                  from: pod
                - tag_name: tib-msg-group-name
                  key: tib-msg-group-name
                  from: pod
            pod_association:
              - sources:
                  - from: resource_attribute
                    name: k8s.pod.uid
          filter/workload:
            metrics:
              include:
                match_type: regexp
                resource_attributes:
                  - key: workload_type
                    value: (user-app|capability-service)$
          transform/metrics:
            metric_statements:
            - context: datapoint
              statements:
                - set(attributes["pod_name"], resource.attributes["k8s.pod.name"])
                - set(attributes["pod_namespace"], resource.attributes["k8s.namespace.name"])
                - set(attributes["app_id"], resource.attributes["app_id"])
                - set(attributes["app_id"], resource.attributes["capability-instance-id"]) where IsMatch(resource.attributes["app_type"], "msg-*")
                - set(attributes["app_type"], resource.attributes["app_type"])
                - set(attributes["dataplane_id"], resource.attributes["dataplane_id"])
                - set(attributes["workload_type"], resource.attributes["workload_type"])
                - set(attributes["app_tags"], resource.attributes["app_tags"])
                - set(attributes["app_name"], resource.attributes["app_name"])
                - set(attributes["app_version"], resource.attributes["app_version"])
                - set(attributes["connectors"], resource.attributes["connectors"])
          filter/include:
            metrics:
              include:
                match_type: regexp
                metric_names:
                  - .*memory.*
                  - .*cpu.*
        exporters:
          prometheus/user:
            endpoint: 0.0.0.0:4319
            enable_open_metrics: true
            resource_to_telemetry_conversion:
              enabled: true
        extensions:
          health_check: {}
          memory_ballast:
            size_in_percentage: 40
        service:
          telemetry:
            logs: {}
            metrics:
              address: :8888
          extensions:
            - health_check
            - memory_ballast
          pipelines:
            logs: null
            traces: null
            metrics:
              receivers:
                - kubeletstats
              processors:
                - k8sattributes/kubeletstats
                - filter/workload
                - filter/include
                - transform/metrics
                - batch
              exporters:
                - prometheus/user
  cluster:
    names:
      - ${TP_CLUSTER_NAME}
  releaseName: otel-collector-daemon
  namespace: prometheus-system
  flags:
    wait: true
    timeout: 1h
    createNamespace: true
