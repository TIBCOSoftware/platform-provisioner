#
# Copyright Â© 2024. Cloud Software Group, Inc.
# This file is subject to the license terms contained
# in the license file that is distributed with this file.
#

pipelineName: "Deploy O11y Stack"
description: |
  Deploy O11y Stack on EKS cluster. This pipeline will create a new EKS cluster and deploy O11y Stack on it.
  For more details see: [link](https://docs.google.com/document/d/1IRZTO5tDh4EbcQUgoD1TZn8yalHH30zdQBYRUKlVAp8/edit?pli=1#heading=h.66aq0wpdolb9)
options:
  - name: "GUI_DP_INGRESS_CLASS"
    labels:
      - "nginx"
      - "traefik"
    values:
      - "nginx"
      - "traefik"
    type: string
    guiType: radio
    reference: "meta.guiEnv.GUI_DP_INGRESS_CLASS"
    description: "Select the ingress class to use"
    required: true
  - name: "GUI_DP_STORAGE_CLASS"
    labels:
      - "hostpath"
      - "standard"
    values:
      - "hostpath"
      - "standard"
    type: string
    guiType: radio
    reference: "meta.guiEnv.GUI_DP_STORAGE_CLASS"
    description: "hostpath for docker desktop, standard for minikube"
    required: true
  - name: "GUI_DP_DOMAIN"
    type: string
    guiType: input
    reference: "meta.guiEnv.GUI_DP_DOMAIN"
    description: "normally we use localhost.dataplanes.pro or localhost as on-perm domain. The service will be like: https://kibana.localhost.dataplanes.pro/ or http://kibana.localhost/"
  - name: "GUI_PIPELINE_LOG_DEBUG"
    type: boolean
    guiType: checkbox
    reference: "meta.guiEnv.GUI_PIPELINE_LOG_DEBUG"
recipe: |
  apiVersion: v1
  kind: helm-install
  meta:
    guiEnv:
      note: "deploy-o11y-stack"
      GUI_PIPELINE_LOG_DEBUG: false
      GUI_DP_INGRESS_CLASS: "nginx"
      GUI_DP_STORAGE_CLASS: "hostpath"
      GUI_DP_DOMAIN: "localhost.dataplanes.pro"
      GUI_DP_CONFIG_CHART_VERSION: "^1.0.0"
      GUI_DP_CONFIG_ES_VERSION: "8.12.1"
      GUI_DP_CONFIG_ES_NAMESPACE: "elastic-system"
      GUI_DP_DEPLOY_ECK: true
      GUI_DP_ECK_VERSION: "2.11.1"
      GUI_DP_KUBE_PROMETHEUS_STACK_VERSION: "57.0.3"
      GUI_DP_OPEN_TELEMETRY_VERSION: "0.74.1"
    globalEnvVariable:
      REPLACE_RECIPE: true
      PIPELINE_LOG_DEBUG: ${GUI_PIPELINE_LOG_DEBUG}
      PIPELINE_CHECK_DOCKER_STATUS: false
      DP_CHART_REPO: https://tibcosoftware.github.io/tp-helm-charts
      DP_DOMAIN: ${GUI_DP_DOMAIN}
      DP_INGRESS_CLASS: ${GUI_DP_INGRESS_CLASS} # nginx, traefik
      DP_STORAGE_CLASS: ${GUI_DP_STORAGE_CLASS} # hostpath for docker desktop, standard for minikube
      DP_ES_RELEASE_NAME: dp-config-es
      DP_CONFIG_CHART_VERSION: "${GUI_DP_CONFIG_CHART_VERSION}"
      DP_CONFIG_ES_VERSION: "${GUI_DP_CONFIG_ES_VERSION}"
      DP_CONFIG_ES_NAMESPACE: "${GUI_DP_CONFIG_ES_NAMESPACE}"
      DP_DEPLOY_ECK: ${GUI_DP_DEPLOY_ECK}
      DP_ECK_VERSION: "${GUI_DP_ECK_VERSION}" 
      DP_KUBE_PROMETHEUS_STACK_VERSION: "${GUI_DP_KUBE_PROMETHEUS_STACK_VERSION}" # https://github.com/prometheus-community/helm-charts/releases
      DP_OPEN_TELEMETRY_VERSION: "${GUI_DP_OPEN_TELEMETRY_VERSION}"
    tools:
      yq: "4.40"
      helm: "3.13"
  helmCharts:
    - name: eck-operator
      version: ${DP_ECK_VERSION}
      condition: ${DP_DEPLOY_ECK}
      repo:
        helm:
          url: https://helm.elastic.co
      cluster:
        names:
          - ${DP_CLUSTER_NAME}
      releaseName: eck-operator
      namespace: elastic-system
      flags:
        wait: true
        timeout: 1h
        createNamespace: true
    - name: dp-config-es
      version: ${DP_CONFIG_CHART_VERSION}
      repo:
        helm:
          url: ${DP_CHART_REPO}
      values:
        keepPrevious: true
        content: |
          domain: ${DP_DOMAIN}
          es:
            version: "${DP_CONFIG_ES_VERSION}"
            ingress:
              ingressClassName: ${DP_INGRESS_CLASS}
              service: ${DP_ES_RELEASE_NAME}-es-http
            storage:
              name: ${DP_STORAGE_CLASS}
          kibana:
            version: "${DP_CONFIG_ES_VERSION}"
            ingress:
              ingressClassName: ${DP_INGRESS_CLASS}
              service: ${DP_ES_RELEASE_NAME}-kb-http
          apm:
            enabled: true
            version: "${DP_CONFIG_ES_VERSION}"
            ingress:
              ingressClassName: ${DP_INGRESS_CLASS}
              service: ${DP_ES_RELEASE_NAME}-apm-http
      cluster:
        names:
          - ${DP_CLUSTER_NAME}
      releaseName: ${DP_ES_RELEASE_NAME}
      namespace: elastic-system
      flags:
        wait: true
        timeout: 1h
        createNamespace: true
    - name: kube-prometheus-stack
      version: "${DP_KUBE_PROMETHEUS_STACK_VERSION}"
      repo:
        helm:
          url: https://prometheus-community.github.io/helm-charts
      values:
        keepPrevious: true
        content: |
          grafana:
            plugins:
              - grafana-piechart-panel
            ingress:
              enabled: true
              ingressClassName: ${DP_INGRESS_CLASS}
              hosts:
              - grafana.${DP_DOMAIN}
          prometheus:
            prometheusSpec:
              enableRemoteWriteReceiver: true
              remoteWriteDashboards: true
              additionalScrapeConfigs:
              - job_name: otel-collector
                kubernetes_sd_configs:
                - role: pod
                relabel_configs:
                - action: keep
                  regex: "true"
                  source_labels:
                  - __meta_kubernetes_pod_label_prometheus_io_scrape
                - action: keep
                  regex: "infra"
                  source_labels:
                  - __meta_kubernetes_pod_label_platform_tibco_com_workload_type
                - action: keepequal
                  source_labels: [__meta_kubernetes_pod_container_port_number]
                  target_label: __meta_kubernetes_pod_label_prometheus_io_port
                - action: replace
                  regex: ([^:]+)(?::\d+)?;(\d+)
                  replacement: $1:$2
                  source_labels:
                  - __address__
                  - __meta_kubernetes_pod_label_prometheus_io_port
                  target_label: __address__
                - source_labels: [__meta_kubernetes_pod_label_prometheus_io_path]
                  action: replace
                  target_label: __metrics_path__
                  regex: (.+)
                  replacement: /$1
            ingress:
              enabled: true
              ingressClassName: ${DP_INGRESS_CLASS}
              hosts:
              - prometheus-internal.${DP_DOMAIN}
      cluster:
        names:
          - ${DP_CLUSTER_NAME}
      releaseName: kube-prometheus-stack
      namespace: prometheus-system
      flags:
        wait: true
        timeout: 1h
        createNamespace: true
    - name: opentelemetry-collector
      version: "${DP_OPEN_TELEMETRY_VERSION}"
      repo:
        helm:
          url: https://open-telemetry.github.io/opentelemetry-helm-charts
      values:
        keepPrevious: true
        content: |
          mode: "daemonset"
          fullnameOverride: otel-kubelet-stats
          podLabels:
            platform.tibco.com/workload-type: "infra"
            networking.platform.tibco.com/kubernetes-api: enable
            egress.networking.platform.tibco.com/internet-all: enable
            prometheus.io/scrape: "true"
            prometheus.io/path: "metrics"
            prometheus.io/port: "4319"
          autoscaling:
            enabled: false
            minReplicas: 1
            maxReplicas: 10
            behavior:
              scaleUp:
                stabilizationWindowSeconds: 15
              scaleDown:
                stabilizationWindowSeconds: 15
            targetCPUUtilizationPercentage: 80
            targetMemoryUtilizationPercentage: 80
          serviceAccount:
            create: true
          clusterRole:
            create: true
            rules:
            - apiGroups: [""]
              resources: ["pods", "namespaces"]
              verbs: ["get", "watch", "list"]
            - apiGroups: [""]
              resources: ["nodes/stats", "nodes/proxy"]
              verbs: ["get"]
          extraEnvs:
            - name: KUBE_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
          ports:
            metrics:
              enabled: true
              containerPort: 8888
              servicePort: 8888
              hostPort: 8888
              protocol: TCP
            prometheus:
              enabled: true
              containerPort: 4319
              servicePort: 4319
              hostPort: 4319
              protocol: TCP
          config:
            receivers:
              kubeletstats:
                collection_interval: 20s
                auth_type: "serviceAccount"
                endpoint: "https://${env:KUBE_NODE_NAME}:10250"
                insecure_skip_verify: true
                metric_groups:
                  - pod
                  - container
                extra_metadata_labels:
                  - container.id
                metrics:
                  k8s.container.memory_limit_utilization:
                    enabled: true
                  k8s.container.cpu_limit_utilization:
                    enabled: true
                  k8s.pod.cpu_limit_utilization:
                    enabled: true
                  k8s.pod.memory_limit_utilization:
                    enabled: true
                  k8s.pod.filesystem.available:
                    enabled: false
                  k8s.pod.filesystem.capacity:
                    enabled: false
                  k8s.pod.filesystem.usage:
                    enabled: false
                  k8s.pod.memory.major_page_faults:
                    enabled: false
                  k8s.pod.memory.page_faults:
                    enabled: false
                  k8s.pod.memory.rss:
                    enabled: false
                  k8s.pod.memory.working_set:
                    enabled: false
            processors:
              memory_limiter:
                check_interval: 5s
                limit_percentage: 80
                spike_limit_percentage: 25
              batch: {}
              k8sattributes/kubeletstats:
                auth_type: "serviceAccount"
                passthrough: false
                extract:
                  metadata:
                    - k8s.pod.name
                    - k8s.pod.uid
                    - k8s.namespace.name
                    - k8s.pod.start_time
                  annotations:
                    - tag_name: connectors
                      key: platform.tibco.com/connectors
                      from: pod
                  labels:
                    - tag_name: app_id
                      key: platform.tibco.com/app-id
                      from: pod
                    - tag_name: app_type
                      key: platform.tibco.com/app-type
                      from: pod
                    - tag_name: dataplane_id
                      key: platform.tibco.com/dataplane-id
                      from: pod
                    - tag_name: workload_type
                      key: platform.tibco.com/workload-type
                      from: pod
                    - tag_name: app_name
                      key: platform.tibco.com/app-name
                      from: pod
                    - tag_name: app_version
                      key: platform.tibco.com/app-version
                      from: pod
                    - tag_name: app_tags
                      key: platform.tibco.com/tags
                      from: pod
                    - tag_name: capability_instance_id
                      key: platform.tibco.com/capability-instance-id
                      from: pod
                    - tag_name: tib_msg_stsrole
                      key: tib_msg_stsrole
                      from: pod
                    - tag_name: tib-msg-group-name
                      key: tib-msg-group-name
                      from: pod
                pod_association:
                  - sources:
                      - from: resource_attribute
                        name: k8s.pod.uid
              filter/workload:
                metrics:
                  include:
                    match_type: regexp
                    resource_attributes:
                      - key: workload_type
                        value: (user-app|capability-service)$
              transform/metrics:
                metric_statements:
                - context: datapoint
                  statements:
                    - set(attributes["pod_name"], resource.attributes["k8s.pod.name"])
                    - set(attributes["pod_namespace"], resource.attributes["k8s.namespace.name"])
                    - set(attributes["app_id"], resource.attributes["app_id"])
                    - set(attributes["app_id"], resource.attributes["capability-instance-id"]) where IsMatch(resource.attributes["app_type"], "msg-*")
                    - set(attributes["app_type"], resource.attributes["app_type"])
                    - set(attributes["dataplane_id"], resource.attributes["dataplane_id"])
                    - set(attributes["workload_type"], resource.attributes["workload_type"])
                    - set(attributes["app_tags"], resource.attributes["app_tags"])
                    - set(attributes["app_name"], resource.attributes["app_name"])
                    - set(attributes["app_version"], resource.attributes["app_version"])
                    - set(attributes["connectors"], resource.attributes["connectors"])
              filter/include:
                metrics:
                  include:
                    match_type: regexp
                    metric_names:
                      - .*memory.*
                      - .*cpu.*
            exporters:
              prometheus/user:
                endpoint: 0.0.0.0:4319
                enable_open_metrics: true
                resource_to_telemetry_conversion:
                  enabled: true
            extensions:
              health_check: {}
              memory_ballast:
                size_in_percentage: 40
            service:
              telemetry:
                logs: {}
                metrics:
                  address: :8888
              extensions:
                - health_check
                - memory_ballast
              pipelines:
                logs: null
                traces: null
                metrics:
                  receivers:
                    - kubeletstats
                  processors:
                    - k8sattributes/kubeletstats
                    - filter/workload
                    - filter/include
                    - transform/metrics
                    - batch
                  exporters:
                    - prometheus/user
      cluster:
        names:
          - ${DP_CLUSTER_NAME}
      releaseName: otel-collector-daemon
      namespace: prometheus-system
      flags:
        wait: true
        timeout: 1h
        createNamespace: true
