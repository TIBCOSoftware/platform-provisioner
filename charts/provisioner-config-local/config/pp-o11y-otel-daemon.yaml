#
# Copyright Â© 2024. Cloud Software Group, Inc.
# This file is subject to the license terms contained
# in the license file that is distributed with this file.
#

pipelineName: "Deploy O11y Stack"
description: |
  This pipeline will deploy OTel collector daemonset.
options:
  - name: "GUI_DP_ENV"
    type: string
    guiType: input
    reference: "meta.guiEnv.GUI_DP_ENV"
    required: true
    description: "This DP env needs to match your certificate. The default cluster name will be dp-cluster-${DP_ENV}"
  - name: "GUI_DP_OTEL_CHART_VERSION"
    type: string
    guiType: input
    reference: "meta.guiEnv.GUI_DP_OTEL_CHART_VERSION"
    description: "The version of opentelemetry-collector helm chart. see: <a href='https://github.com/open-telemetry/opentelemetry-helm-charts/releases' target='_blank'>link</a>"
  - name: "opentelemetry-collector chart values"
    type: string
    guiType: textarea
    reference: "helmCharts[0].values.content"
    lang: "yaml"
  - name: "GUI_PIPELINE_LOG_DEBUG"
    type: boolean
    guiType: checkbox
    reference: "meta.guiEnv.GUI_PIPELINE_LOG_DEBUG"
    lang: "yaml"
recipe: |
  apiVersion: v1
  kind: helm-install
  meta:
    guiEnv:
      note: "deploy-otel-daemon"
      GUI_DP_OTEL_CHART_VERSION: "0.81.1"
      GUI_PIPELINE_LOG_DEBUG: false
    globalEnvVariable:
      REPLACE_RECIPE: true
      PIPELINE_LOG_DEBUG: ${GUI_PIPELINE_LOG_DEBUG}
      PIPELINE_CHECK_DOCKER_STATUS: false
      DP_ENV: ${GUI_DP_ENV}
      DP_CLUSTER_NAME: dp-cluster-${DP_ENV}
      DP_OTEL_CHART_VERSION: "${GUI_DP_OTEL_CHART_VERSION}"
    tools:
      yq: "4.40"
      helm: "3.13"
  helmCharts:
    - name: opentelemetry-collector
      version: "${DP_OTEL_CHART_VERSION}"
      repo:
        helm:
          url: https://open-telemetry.github.io/opentelemetry-helm-charts
      values:
        keepPrevious: true
        content: |
          mode: "daemonset"
          fullnameOverride: otel-kubelet-stats
          podLabels:
            platform.tibco.com/workload-type: "infra"
            networking.platform.tibco.com/kubernetes-api: enable
            egress.networking.platform.tibco.com/internet-all: enable
            prometheus.io/scrape: "true"
            prometheus.io/path: "metrics"
            prometheus.io/port: "4319"
          autoscaling:
            enabled: false
            minReplicas: 1
            maxReplicas: 10
            behavior:
              scaleUp:
                stabilizationWindowSeconds: 15
              scaleDown:
                stabilizationWindowSeconds: 15
            targetCPUUtilizationPercentage: 80
            targetMemoryUtilizationPercentage: 80
          serviceAccount:
            create: true
          extraEnvs:
            - name: KUBE_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
          ports:
            metrics:
              enabled: true
              containerPort: 8888
              servicePort: 8888
              hostPort: 8888
              protocol: TCP
            prometheus:
              enabled: true
              containerPort: 4319
              servicePort: 4319
              hostPort: 4319
              protocol: TCP
          config:
            receivers:
              kubeletstats:
                collection_interval: 20s
                auth_type: "serviceAccount"
                endpoint: "https://${env:KUBE_NODE_NAME}:10250"
                insecure_skip_verify: true
                metric_groups:
                  - pod
                  - container
                extra_metadata_labels:
                  - container.id
                metrics:
                  k8s.container.memory_limit_utilization:
                    enabled: true
                  k8s.container.cpu_limit_utilization:
                    enabled: true
                  k8s.pod.cpu_limit_utilization:
                    enabled: true
                  k8s.pod.memory_limit_utilization:
                    enabled: true
                  k8s.pod.filesystem.available:
                    enabled: false
                  k8s.pod.filesystem.capacity:
                    enabled: false
                  k8s.pod.filesystem.usage:
                    enabled: false
                  k8s.pod.memory.major_page_faults:
                    enabled: false
                  k8s.pod.memory.page_faults:
                    enabled: false
                  k8s.pod.memory.rss:
                    enabled: false
                  k8s.pod.memory.working_set:
                    enabled: false
            processors:
              memory_limiter:
                check_interval: 5s
                limit_percentage: 80
                spike_limit_percentage: 25
              batch: {}
              k8sattributes/kubeletstats:
                auth_type: "serviceAccount"
                passthrough: false
                extract:
                  metadata:
                    - k8s.pod.name
                    - k8s.pod.uid
                    - k8s.namespace.name
                    - k8s.pod.start_time
                  annotations:
                    - tag_name: connectors
                      key: platform.tibco.com/connectors
                      from: pod
                  labels:
                    - tag_name: app_id
                      key: platform.tibco.com/app-id
                      from: pod
                    - tag_name: app_type
                      key: platform.tibco.com/app-type
                      from: pod
                    - tag_name: dataplane_id
                      key: platform.tibco.com/dataplane-id
                      from: pod
                    - tag_name: workload_type
                      key: platform.tibco.com/workload-type
                      from: pod
                    - tag_name: app_name
                      key: platform.tibco.com/app-name
                      from: pod
                    - tag_name: app_version
                      key: platform.tibco.com/app-version
                      from: pod
                    - tag_name: app_tags
                      key: platform.tibco.com/tags
                      from: pod
                    - tag_name: capability_instance_id
                      key: platform.tibco.com/capability-instance-id
                      from: pod
                    - tag_name: app_type
                      key: tib-dp-app
                      from: pod
                    - tag_name: tib-msg-group-name
                      key: tib-msg-group-name
                      from: pod
                pod_association:
                  - sources:
                      - from: resource_attribute
                        name: k8s.pod.uid
              filter/workload:
                metrics:
                  include:
                    match_type: regexp
                    resource_attributes:
                      - key: workload_type
                        value: (user-app|capability-service)$
              transform/metrics:
                metric_statements:
                - context: datapoint
                  statements:
                    - set(attributes["pod_name"], resource.attributes["k8s.pod.name"])
                    - set(attributes["pod_namespace"], resource.attributes["k8s.namespace.name"])
                    - set(attributes["app_id"], resource.attributes["app_id"])
                    - set(attributes["app_type"], resource.attributes["app_type"])
                    - set(attributes["dataplane_id"], resource.attributes["dataplane_id"])
                    - set(attributes["workload_type"], resource.attributes["workload_type"])
                    - set(attributes["app_tags"], resource.attributes["app_tags"])
                    - set(attributes["app_name"], resource.attributes["app_name"])
                    - set(attributes["app_version"], resource.attributes["app_version"])
                    - set(attributes["connectors"], resource.attributes["connectors"])
              filter/include:
                metrics:
                  include:
                    match_type: regexp
                    metric_names:
                      - .*memory.*
                      - .*cpu.*
            exporters:
              prometheus/user:
                endpoint: 0.0.0.0:4319
                enable_open_metrics: true
                resource_to_telemetry_conversion:
                  enabled: true
            extensions:
              health_check: {}
              memory_ballast:
                size_in_percentage: 40
            service:
              telemetry:
                logs: {}
                metrics:
                  address: :8888
              extensions:
                - health_check
                - memory_ballast
              pipelines:
                logs: null
                traces: null
                metrics:
                  receivers:
                    - kubeletstats
                  processors:
                    - k8sattributes/kubeletstats
                    - filter/workload
                    - filter/include
                    - transform/metrics
                    - batch
                  exporters:
                    - prometheus/user
      cluster:
        names:
          - ${DP_CLUSTER_NAME}
      releaseName: otel-collector-daemon
      namespace: prometheus-system
      flags:
        wait: true
        timeout: 1h
        createNamespace: true
